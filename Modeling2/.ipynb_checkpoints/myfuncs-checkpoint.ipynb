{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "# nltk.download()\n",
    "# # download in specific location\n",
    "# nltk.download('all', download_dir='./stopwords')\n",
    "# nltk.download('stopwords')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import OrderedDict\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import PorterStemmer\n",
    "import re\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# read/write json\n",
    "\n",
    "import json\n",
    "import os\n",
    "\n",
    "def JsonUploader(localpath, filename):\n",
    "    with open(os.path.join(localpath, filename)) as pkgnames:\n",
    "        return json.load(pkgnames)\n",
    "\n",
    "def JsonSaver(dictfile, localpath, filename):\n",
    "    with open(os.path.join(localpath, filename), 'w') as outfile:\n",
    "        json.dump(dictfile, outfile)\n",
    "\n",
    "# # write json\n",
    "# temp = copy.deepcopy(json1)\n",
    "# JsonSaver(dictfile=temp, localpath=\"./\", filename=\"temp.json\")\n",
    "# # read json\n",
    "# json2 = JsonUploader(localpath=\"./\", filename=\"temp.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# functions to clean data\n",
    "\n",
    "import unicodedata\n",
    "import mistune\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 'data_raw' to be created in this format\n",
    "# temp = {}\n",
    "# temp['pck1'] = {}\n",
    "# temp['pck1']['txt1'] = txt1\n",
    "# data_raw = copy.deepcopy(temp)\n",
    "\n",
    "# removes stopwords and special chars also\n",
    "def remove_stopwords(sentence):\n",
    "    return \" \".join([word.lower() for word in word_tokenize(sentence)\n",
    "                    if word.lower() not in stop_words and word.isalpha()])\n",
    "\n",
    "def generic_clean(sentence):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(sentence))\n",
    "    # Remove \"_\"\n",
    "    document = document.replace(\"_\", \" \")\n",
    "    # remove unicode characters\n",
    "    document = str(document.encode('ascii','ignore'))\n",
    "    document = document[1:]\n",
    "    document = document[1:]\n",
    "    document = document[:-1]\n",
    "    # remove all numerics\n",
    "    document=''.join(c if c not in map(str,range(0,10)) else \"\" for c in document)\n",
    "    # remove all extra spaces\n",
    "    document = \" \".join(document.split())\n",
    "#     # Remove single characters from the start\n",
    "#     document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "#     # remove all single characters\n",
    "#     document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    return document\n",
    "\n",
    "# need to create \n",
    "def clean_web_data(text_raw):\n",
    "    # readme is like html page\n",
    "    web_text = text_raw\n",
    "    # adds <h>header <p>para <a>anchor <code> html tags\n",
    "    web_text = mistune.markdown(web_text, escape=False)\n",
    "    # renders text like html page (<h> and <p> in separate lines)\n",
    "    soup = BeautifulSoup(web_text, \"html.parser\")\n",
    "    # Replaces anchors with content where relevant and extract otherwise\n",
    "    for link in soup.findAll('a'):\n",
    "        if link.text.startswith('http'):\n",
    "            link.extract() # removes\n",
    "        else:\n",
    "            link.replaceWithChildren()\n",
    "    # Removes all the images\n",
    "    for image in soup.findAll('img'):\n",
    "        image.extract()\n",
    "    # Removes all the code blocks\n",
    "    for code_block in soup.findAll('code'):\n",
    "        code_block.extract()\n",
    "    text_clean = generic_clean(soup.text)\n",
    "    return text_clean\n",
    "\n",
    "def clean_normal_text(text_raw):\n",
    "    text_clean = generic_clean(text_raw)\n",
    "    return text_clean\n",
    "\n",
    "def remove_words(text_raw, word):\n",
    "    text_clean = text_raw.replace(word, \"\")\n",
    "    # remove extra spaces generated by word removal\n",
    "    text_clean = \" \".join(text_clean.split())\n",
    "    return text_clean\n",
    "\n",
    "def clean_unicode(text_raw):\n",
    "    text_clean = str(unicodedata.normalize('NFKD', text_raw).encode('ascii','ignore'))\n",
    "    text_clean = text_clean[1:]\n",
    "    text_clean = text_clean[1:]\n",
    "    text_clean = text_clean[:-1]\n",
    "    return text_clean\n",
    "    # unicode text example: \"Sîne klâwen durh die\"\n",
    "        \n",
    "# Note: data_clean1 not declared as global inside fn, still changes were affected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Stem/Lemmatize with POS Tag\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\n",
    "                \"N\": wordnet.NOUN,        \n",
    "                \"J\": wordnet.ADJ,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV,\n",
    "                }\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "def lemmatize(text1):\n",
    "    # lemmitizes\n",
    "    # works on list of strings\n",
    "    documents = []\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    for sen in text1:      \n",
    "        document = \"\"\n",
    "        for word in sen.split():\n",
    "#             word1 = lemmatizer.lemmatize(word) # only removes 's' at the end of nouns as of now\n",
    "#             word1 = lemmatizer.lemmatize(word, pos =\"a\") # also converts adjective 'better' to 'good'\n",
    "            word1 = lemmatizer.lemmatize(word, get_wordnet_pos(word)) # as per word's POS tag\n",
    "            document = (''.join(document+\" \"+word1)).strip()\n",
    "        documents.append(document)\n",
    "    return documents\n",
    "\n",
    "\n",
    "def stem(text1):\n",
    "    # lemmitizes\n",
    "    # works on list of strings\n",
    "    ps = PorterStemmer()\n",
    "    documents = []\n",
    "    for sen in text1:\n",
    "        words = word_tokenize(sen)\n",
    "        document = []\n",
    "        for w in words:\n",
    "            document.append(ps.stem(w)) # can try other stemming algos to improve accuracy\n",
    "        document = ' '.join(document)\n",
    "        documents.append(document)\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# Rouge - function to find rouge score between 2 strings\n",
    "\n",
    "import rouge\n",
    "\n",
    "def rouge_strings(hypothesis_1, references_1):\n",
    "\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1_score = []\n",
    "\n",
    "    def prepare_results(p, r, f):\n",
    "        return '\\t{}:\\t{}: {:5.2f}\\t{}: {:5.2f}\\t{}: {:5.2f}'.format(metric, 'P', 100.0 * p, 'R', 100.0 * r, 'F1', 100.0 * f)\n",
    "\n",
    "    # def rouge_accuracy():\n",
    "\n",
    "#     for aggregator in ['Avg', 'Best', 'Individual']:\n",
    "    for aggregator in ['Avg']:\n",
    "        print('Evaluation with {}'.format(aggregator))\n",
    "        apply_avg = aggregator == 'Avg'\n",
    "#         apply_best = aggregator == 'Best'\n",
    "\n",
    "        evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                               max_n=3,\n",
    "                               limit_length=True,\n",
    "                               length_limit=100,\n",
    "                               length_limit_type='words',\n",
    "                               apply_avg=apply_avg,\n",
    "#                                apply_best=apply_best,\n",
    "                               alpha=0.5, # Default F1_score # weight for 2*p*r(p+r); gives same val at 0.5\n",
    "                               weight_factor=1.2,\n",
    "                               stemming=True)\n",
    "\n",
    "    #     hypothesis_1 = \"Steve-Jobs was a technology pioneer who created i-phone\" # '-' does not make a difference\n",
    "    #     references_1 = \"Steve-Jobs created best phone named i-phone\"\n",
    "        all_hypothesis = [hypothesis_1]\n",
    "        all_references = [references_1]\n",
    "\n",
    "        scores = evaluator.get_scores(all_hypothesis, all_references)\n",
    "\n",
    "        for metric, results in sorted(scores.items(), key=lambda x: x[0]):\n",
    "            if not apply_avg and not apply_best: # value is a type of list as we evaluate each summary vs each reference\n",
    "                pass\n",
    "            else:\n",
    "#                 print(prepare_results(results['p'], results['r'], results['f']))\n",
    "                precision.append(results['p'])\n",
    "                recall.append(results['r'])\n",
    "                f1_score.append(results['f'])\n",
    "#         print()\n",
    "    \n",
    "    return precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic function:\n",
    "\n",
    "# retrieve name of variable\n",
    "def retrieve_name(var):\n",
    "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
    "    return [var_name for var_name, var_val in callers_local_vars if var_val is var][0]\n",
    "# # Usage:\n",
    "# var_name = {}\n",
    "# retrieve_name(var_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
