{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pytextrank\n",
    "import os\n",
    "import mistune\n",
    "from bs4 import BeautifulSoup\n",
    "import daiquiri\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# links:\n",
    "# https://xang1234.github.io/textrank/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/antrived/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/antrived/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: freeCodeCamp/freeCodeCamp\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'textrank' has no attribute 'extract_key_phrases'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-a001506e0eb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpackage_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnpm_data_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Processing: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpackage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     tags[package_name] = list(textrank.extract_key_phrases(\n\u001b[0m\u001b[1;32m     41\u001b[0m         npm_data_dict[package_name][\"readme_cleaned\"]))\n\u001b[1;32m     42\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tagged: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpackage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'textrank' has no attribute 'extract_key_phrases'"
     ]
    }
   ],
   "source": [
    "daiquiri.setup(level=logging.INFO)\n",
    "logger = daiquiri.getLogger(__name__)\n",
    "\n",
    "stopwords = set([])\n",
    "master_tag_list = set()\n",
    "\n",
    "PATH_PREFIX = 'data/'\n",
    "\n",
    "\n",
    "\n",
    "def load_stopwords():\n",
    "    global stopwords\n",
    "    with open(\"/Users/avgupta/devnull/poc_tagging/custom_stopwords.txt\") as stopwords_file:\n",
    "        stopwords = set(stopwords_file.read().strip().split('\\n'))\n",
    "\n",
    "\n",
    "def execute_stage_one(path_stage0):\n",
    "    path_stage1 = path_stage0.split('::')[0] + \".stage1.output.dat\"\n",
    "    with open(os.path.join(PATH_PREFIX, path_stage1), 'w') as f:\n",
    "        for graf in pytextrank.parse_doc(pytextrank.json_iter(os.path.join(PATH_PREFIX, path_stage0))):\n",
    "            f.write(\"%s\\n\" % pytextrank.pretty_print(graf._asdict()))\n",
    "    return path_stage1\n",
    "\n",
    "\n",
    "def execute_stage_two(path_stage1):\n",
    "    graph, ranks = pytextrank.text_rank(os.path.join(PATH_PREFIX, path_stage1))\n",
    "    pytextrank.render_ranks(graph, ranks)\n",
    "    path_name_components = path_stage1.split('.')\n",
    "    path_name_components[path_name_components.index('stage1')] = 'stage2'\n",
    "    path_stage2 = '-'.join(path_name_components)\n",
    "    with open(os.path.join(PATH_PREFIX, path_stage2), 'w') as f:\n",
    "        for rl in pytextrank.normalize_key_phrases(os.path.join(PATH_PREFIX, path_stage1), ranks, stopwords=stopwords):\n",
    "            f.write(\"%s\\n\" % pytextrank.pretty_print(rl._asdict()))\n",
    "            # to view output in this notebook\n",
    "            # print(pytextrank.pretty_print(rl))\n",
    "    return path_stage2\n",
    "\n",
    "\n",
    "def get_key_phrases(path_stage2):\n",
    "    phrases = set([p for p in pytextrank.limit_keyphrases(os.path.join(PATH_PREFIX, path_stage2), phrase_limit=4)])\n",
    "    print(\"**keywords:** %s\" % (phrases))\n",
    "    return phrases\n",
    "\n",
    "\n",
    "def clean_and_store_readme_sections(npm_data_dict):\n",
    "    for idx, package_name in enumerate(npm_data_dict.keys(), 1):\n",
    "        readme = npm_data_dict[package_name]['readme']\n",
    "        readme_rendered = mistune.markdown(readme, escape=False)\n",
    "        soup = BeautifulSoup(readme_rendered, \"html.parser\")\n",
    "        # Replace anchors with content where relevant and extract otherwise\n",
    "        for link in soup.findAll('a'):\n",
    "            if link.text.startswith('http'):\n",
    "                link.extract()\n",
    "            else:\n",
    "                link.replaceWithChildren()\n",
    "        # Remove all the images\n",
    "        for image in soup.findAll('img'):\n",
    "            image.extract()\n",
    "        # Remove all the code blocks\n",
    "        for code_block in soup.findAll('code'):\n",
    "            code_block.extract()\n",
    "        with open(os.path.join(PATH_PREFIX, '{}::{}.json'.format(package_name.replace('/', ':'), idx)), 'w') as cleaned_readme_store:\n",
    "            cleaned_readme_store.write(\n",
    "                json.dumps({\"id\": idx, \"text\": soup.text}))\n",
    "\n",
    "\n",
    "def run_pipeline(stage0_filename):\n",
    "    logger.info(\"Running pipeline for: \" + stage0_filename)\n",
    "    stage1_filename = execute_stage_one(stage0_filename)\n",
    "    stage2_filename = execute_stage_two(stage1_filename)\n",
    "    return list(get_key_phrases(stage2_filename))\n",
    "\n",
    "\n",
    "def main():\n",
    "    global master_tag_list\n",
    "    load_stopwords()\n",
    "    with open('npm_data.json') as npm_data:\n",
    "        npm_json = json.loads(npm_data.read())\n",
    "    clean_and_store_readme_sections(npm_json)\n",
    "    keyphrase_list = {}\n",
    "    for json_file in os.listdir(PATH_PREFIX):\n",
    "        try:\n",
    "            package_full_name = json_file.split('::')[0].replace(\":\", \"/\")\n",
    "            keyphrase_list.setdefault(package_full_name, run_pipeline(json_file))\n",
    "            master_tag_list = master_tag_list.union(set(keyphrase_list.get(package_full_name, [])))\n",
    "        except Exception as e:\n",
    "            logger.warning(\"Could not tag:\" + json_file)\n",
    "            logger.debug(e)\n",
    "    with open(os.path.join(PATH_PREFIX, 'final_result.json'), 'w') as final_result_outfile:\n",
    "        final_result_outfile.write(json.dumps(keyphrase_list))\n",
    "    with open(os.path.join(PATH_PREFIX, 'master_tag_list.json'), 'w') as master_tag_list_file:\n",
    "        master_tag_list_file.write(json.dumps(list(master_tag_list)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(stopwords.words(\"english\"))\n",
    "global master_tag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
