{"aflc/editdistance": {"git_readme": "editdistance fast implementation of the edit distance levenshtein distance this library simply implements with and cython the algorithm used in this library is proposed by binary wheels thanks to there are binary wheels on linux mac os and windows install you can install via pip code block bash usage it quite simple code block python simple benchmark with ipython tried several libraries on python code block python distance with any object above libraries only support strings but sometimes other type of objects such as list of strings words support any iterable only requires hashable object of it code block python so if object hash is same it same you can provide method to your object instances enjoy license it is released under the mit license", "pypi_description": "", "reference_keywords": ""}, "omerucel/basitapi": {"git_readme": "basitapi restful api oluturmanzda size yardmc olan basit bir ktphane django nun snf temelli view yaps zerine kurulu bir restful api de olmas gereken baz temel zellikleri sunar ve baka da bir ie karmaz sunduu baz temel zellikler balantdaki method parametresi ile http method balnn ezilmesi baz istemciler sadece baz metodlar gndermekle kstlandrlabilirler bu trl bir durumda api nin doru alabilmesi iin balant iinde method isimli parametreyi kullanabilirsiniz bu parametre ile http method ezilir ve gnderdiiniz method parametresindeki deere gre ilemler yrtlr her zaman http kodu ile dn baz istemciler sadece baz hata kodlarna yant verebilirler basitapi bu tr durumlar iin suppress response codes parametresine destek vermekte bu parametre deeri olarak gnderildiinde durumu ne olursa olsun tm yantlar kodu ile gnderilmekte yant format hem accept balk bilgisine hem de balant dosya son ekine gre yant format ayarlanabilmekte kurulum pypi zerinden kurulum iin aadaki komutu kullanabilirsiniz github zerinden kurulum iin aadaki komutu kullanabilirsiniz kullanm settings py urls py balant dosya son ekine gre yant formatn ayarlamak istemiyorsanz balantlarnz format suffix patterns ile tekrar formatlamanza gerek yok views py", "pypi_description": "basitapi restful api oluturmanzda size yardmc olan basit bir ktphane django nun snf temelli view yaps zerine kurulu bir restful api de olmas gereken baz temel zellikleri sunar ve baka da bir ie karmaz sunduu baz temel zellikler balantdaki method parametresi ile http method balnn ezilmesi baz istemciler sadece baz metodlar gndermekle kstlandrlabilirler bu trl bir durumda api nin doru alabilmesi iin balant iinde method isimli parametreyi kullanabilirsiniz bu parametre ile http method ezilir ve gnderdiiniz method parametresindeki deere gre ilemler yrtlr her zaman http kodu ile dn baz istemciler sadece baz hata kodlarna yant verebilirler basitapi bu tr durumlar iin suppress response codes parametresine destek vermekte bu parametre deeri olarak gnderildiinde durumu ne olursa olsun tm yantlar kodu ile gnderilmekte yant format hem accept balk bilgisine hem de balant dosya son ekine gre yant format ayarlanabilmekte kurulum pypi zerinden kurulum iin aadaki komutu kullanabilirsiniz github zerinden kurulum iin aadaki komutu kullanabilirsiniz kullanm settings py urls py balant dosya son ekine gre yant formatn ayarlamak istemiyorsanz balantlarnz format suffix patterns ile tekrar formatlamanza gerek yok views py", "reference_keywords": "rest api django"}, "timofurrer/pysingleton": {"git_readme": "pysingleton module which provides decorator to create thread safe singleton classes version abandoned please don use this anymore author timo furrer tuxtimo gmail com version know know that singletons are evil how to install install it with use use it in your python programs with", "pypi_description": "", "reference_keywords": ""}, "nhfruchter/pgh-bustime": {"git_readme": "pghbustime this time it official an interface to port authority official api yes you need an api key register for an account on the port authority site examples setup using raw api output if you want just your standard json output just use the api which is wrapper around the module get info about busses all stops on the route next bus at stop east liberty station stop a more object oriented interface current location of all busses on route info on particular bus you can chain everything together too find the next bus at the outbound negley stop note on caching caching the results of your api queries in an expiring local store of some sort is highly recommended due to the api somewhat restrictive initial limits highly recommend python lru cache module or", "pypi_description": "", "reference_keywords": ""}, "TUW-GEO/repurpose": {"git_readme": "repurpose this package provides routines for the conversion of image formats to time series and vice versa it is part of the and works best with the readers and writers supported there the main use case is for data that is sampled irregularly in space or time if you have data that is sampled in regular intervals then there are alternatives to this package which might be better for your use case see for more detail the readers and writers have to conform to the api specifications of the base classes defined in to work without adpation citation if you use the software in publication then please cite it using the zenodo doi be aware that this badge links to the latest package version please select your specific version at to get the doi of that version you should normally always use the doi for the specific version of your record in citations this is to ensure that other researchers can access the exact research artefact you used for reproducibility you can find additional information regarding doi versioning at installation this package should be installable through pip code modules it includes two main modules for image swath to time series conversion including support for spatial resampling for time series to image conversion including support for temporal resampling this module is very experimental at the moment alternatives if you have data that can be represented as d datacube then these projects might be better suited to your needs is package that works with netcdf input and output and converts time slices into time series representation can work with several input formats stack them and change the chunking to allow time series optimized access it assumes regular sampling in space and time as far as we know are similar to cdo with stronger focus on netcdf contribute we are happy if you want to contribute please raise an issue explaining what is missing or if you find bug we will also gladly accept pull requests against our master branch for new features or bug fixes development setup for development we recommend environment guidelines if you want to contribute please follow these steps fork the repurpose repository to your account make new feature branch from the repurpose master branch add your feature please include tests for your contributions in one of the test directories we use py test so simple function called test my feature is enough submit pull request to our master branch note this project has been set up using pyscaffold for details and usage information on pyscaffold see", "pypi_description": "repurpose this package provides routines for the conversion of image formats to time series and vice versa it is part of the and works best with the readers and writers supported there the main use case is for data that is sampled irregularly in space or time if you have data that is sampled in regular intervals then there are alternatives to this package which might be better for your use case see for more detail the readers and writers have to conform to the api specifications of the base classes defined in to work without adpation citation if you use the software in publication then please cite it using the zenodo doi be aware that this badge links to the latest package version please select your specific version at to get the doi of that version you should normally always use the doi for the specific version of your record in citations this is to ensure that other researchers can access the exact research artefact you used for reproducibility you can find additional information regarding doi versioning at installation this package should be installable through pip code modules it includes two main modules for image swath to time series conversion including support for spatial resampling for time series to image conversion including support for temporal resampling this module is very experimental at the moment alternatives if you have data that can be represented as d datacube then these projects might be better suited to your needs is package that works with netcdf input and output and converts time slices into time series representation can work with several input formats stack them and change the chunking to allow time series optimized access it assumes regular sampling in space and time as far as we know are similar to cdo with stronger focus on netcdf contribute we are happy if you want to contribute please raise an issue explaining what is missing or if you find bug we will also gladly accept pull requests against our master branch for new features or bug fixes development setup for development we recommend environment guidelines if you want to contribute please follow these steps fork the repurpose repository to your account make new feature branch from the repurpose master branch add your feature please include tests for your contributions in one of the test directories we use py test so simple function called test my feature is enough submit pull request to our master branch note this project has been set up using pyscaffold for details and usage information on pyscaffold see", "reference_keywords": "routines for the conversion of image formats to time series and vice versa"}, "mverteuil/widget-party": {"git_readme": "widget party django dashing widget pack django dashing quick start install the latest stable version from pypi add widget party to your installed apps setting like this include the widgets you like to use to your django dashing entry done demo turn key demo is available at you can use the demo to help choose which widgets you like to use play with them and quickly determine their suitability and limits image image", "pypi_description": "widget party django dashing widget pack django dashing quick start install the latest stable version from pypi add widget party to your installed apps setting like this include the widgets you like to use to your django dashing entry done demo turn key demo is available at you can use the demo to help choose which widgets you like to use play with them and quickly determine their suitability and limits image image", "reference_keywords": ""}, "/reggie": {"git_readme": "", "pypi_description": "reggie the registration and management system downloads and docs full documentation is available on built packages are available on is available on feel free to to request feature or report bug and make changes to the master branch for next release send pull request and pester the maintainer until it merged make sure to add yourself to and update build status alt build status alt coverage status alt documentation status", "reference_keywords": ""}, "crs-support/ftw": {"git_readme": "framework for testing wafs ftw purpose this project was created by researchers from modsecurity and fastly to help provide rigorous tests for waf rules it uses the owasp core ruleset as baseline to test rules on waf each rule from the ruleset is loaded into yaml file that issues http requests that will trigger these rules users can verify the execution of the rule after the tests are issued to make sure the expected response is received from an attack goals use cases include find regressions in waf deployments by using continuous integration and issuing repeatable attacks to waf provide testing framework for new rules into modsecurity if rule is submitted it must have corresponding positive negative tests evaluate wafs against common agreeable baseline ruleset owasp test and verify custom rules for wafs that are not part of the core rule set for our release announcement check out the owasp crs blog installation writing your first tests the core of ftw is it extensible based tests this section lists few resources on how they are formatted how to write them and how you can use them owasp crs wrote great blog post describing how ftw tests are written and executed yamlformat md is ground truth of all fields that are currently understood by ftw after reading these two resources you should be able to get started in writing tests you will most likely be checking against status code responses or web request responses using the directive for integrating ftw to test regexes within your waf logs refer to extendingftw md provisioning apache modsecurity owasp crs if you require an environment for testing waf rules there has been one created with apache modsecurity and version of the owasp core ruleset this can be deployed by checking out the repository typing", "pypi_description": "", "reference_keywords": "framework for testing wafs ftw waf modsecurity owasp security tools waf"}, "/path": {"git_readme": "", "pypi_description": "", "reference_keywords": ""}, "/steelscript-stock": {"git_readme": "", "pypi_description": "stock report for steelscript application framework steelscript is collection of libraries and scripts in python and javascript for interacting with riverbed technology devices this package demonstrates how an external data source can be incorporated as plugin to app framework with associated reports for complete guide to installation see", "reference_keywords": ""}, "eofs/aws": {"git_readme": "utility program for amazon web services features command line interface for boto wrapper for fabric list instances and regions for elb and ec services list ec instances for specific elb manage your ec instances run fabric tasks against all ec instances or ec instances of specific elb motivation we all love boto and fabric why not combine them goal of this project is to provide easy to use command line interface for boto and provide tools for finding out hosts for your fabric remote calls are you using fabric now you don have to write code to find your amazon ec instances by yourself just pass the name of your elastic load balancer and name of your custom fabric function and your servers are updated todo more features start stop terminate instances manage your amis aws auto scaling support installation configuration should work with minimal configuration you only need to make sure that and environment variables are set if you want to use fabric and execute code on your remote servers or use different regions you can set necessary info into settings file see boto documentation for more how to set amazon access credentials see fabric documentation on how to set ssh access credentials usage examples see for more information or for command specific help list all elb instances list all ec instances list all ec instances for elb named mybalancer run fabric tasks against ec instances run fabric tasks against ec instances and define fabfile to be used you can pass parameters to your methods as with fabric command or run fabric tasks against ec instances of specific elb", "pypi_description": "", "reference_keywords": ""}, "akoumjian/datefinder": {"git_readme": "datefinder extract dates from text alt travis build status alt pypi downloads per day alt pypi version alt gitter chat python module for locating dates inside text use this package to extract all sorts of date like strings from document and turn them into datetime objects this module finds the likely datetime strings and then uses to convert to the datetime object installation code block sh how to use automodule datefinder members find dates code block python support you can talk to us on or just submit an issue on", "pypi_description": "datefinder extract dates from text alt travis build status alt pypi downloads per day alt pypi version alt gitter chat python module for locating dates inside text use this package to extract all sorts of date like strings from document and turn them into datetime objects this module finds the likely datetime strings and then uses to convert to the datetime object installation code block sh how to use automodule datefinder members find dates code block python support you can talk to us on or just submit an issue on", "reference_keywords": "datetime parser nlp"}, "mobiusklein/glypy": {"git_readme": "glypy glycan analysis and glycoinformatics library for python glycobiology is the study of the biological functions properties and structures of carbohydrate biomolecules also called glycans these large tree like molecules are complex having wide variety of building blocks as well as modifications and substitutions on those building blocks much in the same way other bioinformatics libraries provide ways to represent dna rna or protein sequences this library attempts to provide representation of glycans much of the variation found in the building blocks of these structures monosaccharides are caused by substitutions of functional groups on common core structure features read in and write out common glycan structure formats and sources glycoct condensed o glycoct xml glycominds linear code o iupac three letter code o retreive data from using the web services provided by glytoucan or run queries directly on their triplestore manipulate glycan structures traverse structures using either canonical or residue level rule ordering operate on monosaccharide and substituents as nodes and bonds as edges add remove and modify these structures to alter glycan properties identify substructures and motifs classifying glycans evaluate structural similarities with one of several ordering and comparator methods plot tree structures with matplotlib rendering using configurable symbol nomenclature such as snfg cfg or iupac layout using vector graphics for perfect scaling example use cases calculate the mass of native or derivatized glycan generate glycosidic and cross ring cleavage fragments for collection of glycan structures for performing ms ms database search perform substructure similarity searches with exact ordering or topological comparison and exact or fuzzy per residue matching to classify structure as an linked glycan annotate ms spectra with glycan structures labeling which peaks matched database entry download all glycans from glytoucan find all glycans in list which contain particular subtree or find common subtrees in database of glycans performing treelet enrichment analysis synthesize all possible glycans using set of enzymes starting from set of seed structures", "pypi_description": "", "reference_keywords": "glycomics glycan carbohydrate glycoinformatics glypy linked linked glycosaminoglycan"}, "/pyfuzz": {"git_readme": "", "pypi_description": "pyfuzz pyfuzz is simple light weight library used for randomly generating number of common file types examples can be found in examples examples py current functions includes ascii utf png jpg gif wide number of languages including chinese han japanese and russian raw bytes regex specified string groups full language list chinese han dutch english finnish french german greek hebrew italian japanese latin polish portugese russian serbian spanish", "reference_keywords": ""}, "mercadopago/mercadopago": {"git_readme": "", "pypi_description": "mercadopago sdk module for payments integration install on python on python basic checkout configure your credentials get your access token in the following address argentina brazil mexico venezuela colombia create payment get customer view more custom checkout related apis in developers site argentina brazil mexico venezuela colombia generic methods you can access any other resource from the mercadopago api using the generic methods for example if you want to get the sites list no params and no authentication running tests on python on python x", "reference_keywords": "api mercadopago checkout payment ipn sdk integration"}, "samuelcolvin/devtools": {"git_readme": "", "pypi_description": "python devtools buildstatus coverage pypi dev tools for python the debug print command python never had and other things for more information see install just is not required but if it available output will be highlighted and easier to read usage code python from devtools import debug whatever debug whatever outputs test py whatever list that only the tip of the iceberg for example code python import numpy as np data foo np array range bar apple banana carrot grapefruit spam i i for in range for in range sentence this is just boring sentence debug data outputs image align center usage without import modify making available in any python code code python add devtools debug to builtins try from devtools import debug except importerror pass else builtins debug debug buildstatus coverage pypi", "reference_keywords": ""}, "kelsoncm/python-brfied": {"git_readme": "python brfied why create brfied because localflavors dont validate user data and dont apply mask on inputs license the mit license mit copyright kelsoncm permission is hereby granted free of charge to any person obtaining copy of this software and associated documentation files the software to deal in the software without restriction including without limitation the rights to use copy modify merge publish distribute sublicense and or sell copies of the software and to permit persons to whom the software is furnished to do so subject to the following conditions the above copyright notice and this permission notice shall be included in all copies or substantial portions of the software the software is provided as is without warranty of any kind express or implied including but not limited to the warranties of merchantability fitness for particular purpose and noninfringement in no event shall the authors or copyright holders be liable for any claim damages or other liability whether in an action of contract tort or otherwise arising from out of or in connection with the software or the use or other dealings in the software", "pypi_description": "", "reference_keywords": "python br brazil brasil model form locale"}, "stdlib/lib": {"git_readme": "standard library is an api development hosting and integration platform standard library setup node python ruby web introduction standard library is the fastest easiest way to build infinitely scalable self healing apis the standard library platform consists of three components central registry and library for apis scalable serverless hosting platform simple command line tooling for building and managing api development standard library is based on function as service serverless architecture initially popularized by aws lambda you can use standard library to build modular scalable apis for yourself and other developers in minutes without having to manage servers gateways domains write documentation or build sdks your development workflow has never been easier focus on writing code you love let standard library handle everything else standard library uses an open specification called faaslang for function definitions and execution if you run into concerns or questions as you re building from this guide please reference the faaslang repository you can view services published by our large and growing developer community on the standard library search page table of contents getting started creating your first service connecting service endpoints accessing your apis from other applications accessing your apis over http running your apis as background workers version control and package management logging sourcecode additional functionality acknowledgements contact getting started to get started with standard library first make sure you have node installed available from the official node js website next install the standard library cli tools with and you re now ready to start building upgrading from previous versions if you re running previous version of standard library and having issues with the cli try cleaning up the old cli binary links first creating your first service the first thing you ll want to do is create workspace create new directory you intend to build your services in and initialize the workspace you ll be asked for an mail address to log in to the standard library registry if you don yet have an account you can create one by going to note that you can skip account creation with you ll be unable to use the registry but it useful for creating workspaces when you don have internet access next create your service you ll be asked for default function name which is the entry point into your service useful if you only want single entry point this will automatically generate service project scaffold in once created enter the service directory in this directory you ll see something like at this point there a hello world function that been automatically created standard library comes paired with simple command for testing your functions locally and running them in the cloud to test your function if we examine the file we see the following we can pass parameters to it using the cli by specifying named parameters note that is magic parameter automatically populated with execution details when provided as is terminates execution so these don need to be documented and can not be specified as parameters when executing the function pushing to the cloud to push your function to development environment in the cloud and to release it when you re ready you can check out your service on the web and use it in applications using our functions gateway that it you haven written line of code yet and you have mastery over building service testing it in development staging environment online and releasing it for private or public consumption note by default apis that you publish with will have visible documentation page in the standard library public registry you can keep your page private as well as restrict execution access or add collaborators to your api by modifying your api permissions for more information see this docs page another note staging environments like the one created with are mutable and can be replaced indefinitely releases are immutable and can never be overwritten however any service can be torn down with or but releases can be replaced once removed to prevent mistakes and or bad actors connecting service endpoints you ll notice that you can create more than one function per service while you can structure your project however you like internally it should also be noted that these functions have zero latency access to each other you can access them internally with the package on npm which behaves similarly to the command for testing use in your main service directory to add it and use it like so functions add js functions add double js in this case calling will return and will return the magic parameter is used for its property which will return the string in the case of local execution when deployed to an environment or release where is your environment name or semver accessing your apis from other applications as mentioned in the previous section you can use the npm package that available on github and npm to access your apis from legacy node js applications and even the web browser we ll have more sdks coming out in the following months an existing app would call function username besttrekchar with version which would speak to your api accessing your apis over http we definitely recommend using the lib library on npm to make api calls as specified above but you can also make https requests directly to the standard library gateway http query parameters are mapped automatically to parameters by name maps directly to running your apis as background workers to run any standard library service as background worker immediately returns response runs function after simply append bg to the url before the http query parameters search portion of the url for example from above to do so from the library use background responses the default background response will be content type of with string indicating the function name you re executing there are currently three different options for background responses that you define before you deploy your function info default set in your comment definition like so this is the default as well if nothing is specified empty set in your comment definition like so will return an empty length response params set in your comment definition like so this will return in this example if no other parameters are specified as this parameter has default value this will spit back any and all parameters sent to the function even if they re not part of the function signature version control and package management quick note on version control standard library is not replacement for normal git based workflows it is supplement focused around service creation and execution you have unlimited access to any release that hasn been torn down with to download the tarball and to automatically download and unpack the tarball to working directory tarballs and package contents are closed source nobody but you and potentially your teammates has access to these it up to you whether or not you share the guts of your service with others on github or npm as mentioned above releases are immutable and can not be overwritten but can be removed just not replaced afterwards and development staging environments are mutable you can overwrite them as much as you like logging logging for services is enabled by default when running service locally with or all logs will be output in your console the very last output normally json compatible string is the return value of the function to view remote logs in dev or release environments use the following syntax the default log type is though you can specify with limit the number of lines to show with the argument or sourcecode standard library sourcecode is designed to streamline the creation of different types of projects sources provide defaults for things like boilerplate code workflows and directory setup so you can get right to development and implementation of more complex functionality you can create services from existing source codes or create and share your own sources installing service from sourcecode you can create service from source code directly from the command line to create service using source code navigate to standard library root directory and run where is something like with an optionally specified version or environment this will create new service based off the source code creating sources to turn existing service into source code navigate to the service and run this will copy the current directory contents into new folder and add file based off of the file to deploy draft of the source code to the cloud you can run to publish versioned immutable source code to the registry you can run you can also fork an existing source code that belongs to you teammate or is publicly available with which you can then modify and publish again under your own account for more information about source codes check out the docs additional functionality standard library comes packed with bunch of other goodies if your service goes down for any reason the service platform is acting up use similarly as we roll out updates to the platform the builds we re using on aws lambda may change you can update your service to our latest build using we may recommend this from time to time so pay attention to mails and the community to see full list of commands available for the cli tools type we ve conveniently copy and pasted the output here for you to peruse that it yep it really that easy to keep up to date on developments please star us here on github and sign up user account for the registry you can read more about service hosting and keep track of official updates on the official standard library website stdlib com acknowledgements standard library is product of and polybit inc we love for you to pay attention to stdlibhq and what we re building next if you consider joining the team shoot us an mail you can also follow me the original author on twitter keithwhor issues encouraged prs welcome and we re happy to have you on board enjoy and happy building thanks special thanks to the people and companies that have believed in and supported our vision and development over the years slack slackhq stripe stripe romain huet romainhuet chad fowler chadfowler brian leroux brianleroux ahmad nassri ahmadnassri and many more", "pypi_description": "", "reference_keywords": "standard library api development deployment and management tools registry serverless faas nodejs node stdlib sdks aws lambda api stdlib microservice serverless faas lib"}, "nioinnovation/xbee": {"git_readme": "", "pypi_description": "xbee build status badge pypi badge py versions badge build status badge pypi badge py versions badge xbee provides an implementation of the xbee serial communication api it allows one to easily access advanced features of one or more xbee devices from an application written in python an example use case might look like this code python installation install from source extract the source code to your computer then run the following command in the root of the source tree this will automatically install the package for you install with tornado support documentation see the python xbee project on to build the documentation yourself ensure that is installed then cd into the docs folder and run make html the documentation can then be opened in any modern web browser at docs build html index html for more information about building or modifying this project documentation see the documentation for the sphinx project dependencies pyserial additional dependencies if wanting to use the tornado ioloop to run automated tests to build the documentation xbee firmware please ensure that your xbee device is programmed with the latest firmware provided by digi using an old firmware revision is not supported and may result in unspecified behavior contributors paul malmsten pmalmsten gmail com greg rapp gdrapp gmail com brian blalor bravo org chris brackert cbrackert gmail com amit synderman marco sangalli james saunders james saunders family net david walker dwalker io", "reference_keywords": ""}, "ramses-tech/ra": {"git_readme": "ra is test suite generator and helper library for testing apis described in raml out of the box ra provides basic automated test suite to test the routes declared in the raml document it provides test helpers for augmenting these with custom tests to test application specific logic side effects etc ra is primarily designed to provide testing support for ramses and nefertari applications but can be used with any wsgi conformant raml described api it currently depends on pytest but may be adapted for other test frameworks in the future it works best using webtest but doesn require it name ra was the god of the sun the most important god in ancient egypt", "pypi_description": "ra is test suite generator and helper library for testing apis described in raml out of the box ra provides basic automated test suite to test the routes declared in the raml document it provides test helpers for augmenting these with custom tests to test application specific logic side effects etc ra is primarily designed to provide testing support for ramses and nefertari applications but can be used with any wsgi conformant raml described api it currently depends on pytest but may be adapted for other test frameworks in the future it works best using webtest but doesn require it name ra was the god of the sun the most important god in ancient egypt try it dev run the test suite check out the example the example raml at and the test file should be helpful to reference see the docs in and the and in", "reference_keywords": "ra is test suite generator and helper library for testing apis described in raml testing web raml"}, "magmax/colorize": {"git_readme": "colorize give some color to your remote tty version downloads tests coverage wheel pip version pip downloads travis coveralls wheel and it is free checkout the installation and usage two options to install it in your system project and you can use it with now you have two ways to use it rendering the output other way to use it this method can do disgusting things with too long outputs options you can change the output format with the argument code or code it uses the same format that so you can use any of its special variables like code to show the time code to show the message itself code to show the relative time you can combine them as you wish example default date format is code but you can change it with code configuration file it will find configuration file in the current directory in the home directory or in the default path directory the first one found will be used so it will search for the format for this file is very easy it is csv file with next fields for example you can configure it to colorize the output available colors code code code code code code code code code and that all example to simulate colordiff to emulate colordiff just use this configuration file that enough travis alt travis results coveralls alt coveralls results pip version alt latest pypi version pip downloads alt number of pypi downloads wheel alt wheel status travis coveralls project download the lastest egg source code any other output format allowed by logging", "pypi_description": "colorize give some color to your remote tty version downloads tests coverage wheel pip version pip downloads travis coveralls wheel and it is free checkout the installation and usage two options to install it in your system project and you can use it with now you have two ways to use it rendering the output other way to use it this method can do disgusting things with too long outputs options you can change the output format with the argument code or code it uses the same format that so you can use any of its special variables like code to show the time code to show the message itself code to show the relative time you can combine them as you wish example default date format is code but you can change it with code configuration file it will find configuration file in the current directory in the home directory or in the default path directory the first one found will be used so it will search for the format for this file is very easy it is csv file with next fields for example you can configure it to colorize the output available colors code code code code code code code code code and that all example to simulate colordiff to emulate colordiff just use this configuration file that enough travis alt travis results coveralls alt coveralls results pip version alt latest pypi version pip downloads alt number of pypi downloads wheel alt wheel status travis coveralls project download the lastest egg source code any other output format allowed by logging", "reference_keywords": "interface"}, "/null": {"git_readme": "", "pypi_description": "null implements the provides singleton that can be used like but is not and is not equal to that returns instead of raising that returns instead of raising routine that converts mappings and sequences to the nullified variant an singleton for clearing up apis to distinguish between keyword argument that is set by the user as and simply not set by the user how do run the tests the easiest way would be to extract the source tarball and run python test test null py change log long overdue python support initial published version authors glued together by", "reference_keywords": ""}, "/js-knockout": {"git_readme": "", "pypi_description": "js knockout introduction this library packages for this requires integration between your web framework and and making sure that the original resources shipped in the directory in are published to some url how to use you can import from and it where you want these resources to be included on page from js knockout import knockout knockout need changes updated knockout version to current initial release", "reference_keywords": ""}, "peeringdb/django-peeringdb": {"git_readme": "django peeringdb peeringdb models and local synchronization for django see the docs at", "pypi_description": "", "reference_keywords": "django peeringdb models"}, "unt-libraries/pyuntl": {"git_readme": "pyuntl python module for reading and writing untl metadata records license see license txt acknowledgements pyuntl was developed at the unt libraries and has been worked on by number of developers over the years including brandon fredericks kurt nordstrom joey liechty lauren ko mark phillips if you have questions about the project feel free to contact mark phillips at mark phillips unt edu", "pypi_description": "see the home page for more information", "reference_keywords": "python package for reading and writing untl metadata records  untl metadata digital libraries records"}, "matthiask/pdfdocument": {"git_readme": "pdfdocument this is wrapper for reportlab which allows easy creation of pdf documents letters and reports pdfdocument comes with two different pdf templates letters and reports the only difference is the layout of the first page the letter has an additional frame for the address at the top and smaller main content area usage is as follows the letter generates default styles using point fonts as base size the report uses points this can be changed by calling again there exists also special type of report the confidential report the only differences being that the confidentiality is marked using red cross at the top of the first page and watermark in the background styles the call to generates set of predefined styles yes it does that includes the following styles this list is neither exhaustive nor promise most of the time you will not use those attributes directly except in the case of tables convenience methods exist for almost all styles as described in the next chapter content all content passed to the following methods is escaped by default reportlab supports html like markup language if you want to use it directly you ll have to either use only or resort to creating instances by hand headings paragraphs unordered lists mini html various elements tables canvas methods canvas methods work with the canvas directly and not with platypus objects they are mostly useful inside stationery functions you ll mostly use reportlab canvas methods directly and only resort to the following methods for special cases additional methods django integration pdfdocument has few helpers for generating pdfs in django views most notably the svg support uses svglib by dinu gherman it can be found on pypi", "pypi_description": "pdfdocument this is wrapper for reportlab which allows easy creation of pdf documents letters and reports pdfdocument comes with two different pdf templates letters and reports the only difference is the layout of the first page the letter has an additional frame for the address at the top and smaller main content area usage is as follows the letter generates default styles using point fonts as base size the report uses points this can be changed by calling again there exists also special type of report the confidential report the only differences being that the confidentiality is marked using red cross at the top of the first page and watermark in the background styles the call to generates set of predefined styles yes it does that includes the following styles this list is neither exhaustive nor promise most of the time you will not use those attributes directly except in the case of tables convenience methods exist for almost all styles as described in the next chapter content all content passed to the following methods is escaped by default reportlab supports html like markup language if you want to use it directly you ll have to either use only or resort to creating instances by hand headings paragraphs unordered lists mini html various elements tables canvas methods canvas methods work with the canvas directly and not with platypus objects they are mostly useful inside stationery functions you ll mostly use reportlab canvas methods directly and only resort to the following methods for special cases additional methods django integration pdfdocument has few helpers for generating pdfs in django views most notably the svg support uses svglib by dinu gherman it can be found on pypi", "reference_keywords": ""}, "syrusakbary/djinja": {"git_readme": "djinja djinja tries to integrate jinja in django the aim is to replace completely the django template system including administration currently the following templating modules have been written and are working django administration django debug toolbar in near future you could convert django templates syntax to jinja syntax isn awesome if you have ideas please let us know installation add the directory to your python path add the following template loader at top of template loaders to your project file tying into template loaders allows djinja to manage automatically all the templates with jinja including django templates note if you don install djinja contrib admin when you try to access to the django administration you will get an error this is caused because the django administration templates are not adapted for jinja that all important you have to adapt your website templates to jinja or you will get an error when rendering until the djinja converser is ready custom filters and extensions djinja uses the same templatetag library approach as django meaning your app has directory and each of it modules represents template library providing new filters and tags custom class in can be used to register jinja specific components djinja can automatically make your existing django filters usable in jinja but not your custom tags you need to rewrite those as jinja extensions manually example for jinja enabled template library you may also define additional extensions filters tests and globas via your haml djinja can render haml pages having installed and is as just simple as put the haml extension to your template and adding in the jinja extensions variable of your settings jinja extensions djinja template extensions haml haml templates can also include extend etc html templates and viceversa administration for install the django administration jinja templating just add before django contrib admin in your installed apps in example configuration benchmarking running tests in django with jinja djinja performance using jinja instead of django templating in administration django debug toolbar for install the django debug toolbar jinja templating just add before debug toolbar in your installed apps in example configuration todos and bugs see", "pypi_description": "djinja djinja tries to integrate jinja in django the aim is to replace completely the django template system including administration currently the following templating modules have been written and are working django administration django debug toolbar in near future you could convert django templates syntax to jinja syntax isn awesome if you have ideas please let us know installation add the directory to your python path add the following template loader at top of template loaders to your project file tying into template loaders allows djinja to manage automatically all the templates with jinja including django templates note if you don install djinja contrib admin when you try to access to the django administration you will get an error this is caused because the django administration templates are not adapted for jinja that all important you have to adapt your website templates to jinja or you will get an error when rendering until the djinja converser is ready custom filters and extensions djinja uses the same templatetag library approach as django meaning your app has directory and each of it modules represents template library providing new filters and tags custom class in can be used to register jinja specific components djinja can automatically make your existing django filters usable in jinja but not your custom tags you need to rewrite those as jinja extensions manually example for jinja enabled template library you may also define additional extensions filters tests and globas via your haml djinja can render haml pages having installed and is as just simple as put the haml extension to your template and adding in the jinja extensions variable of your settings jinja extensions djinja template extensions haml haml templates can also include extend etc html templates and viceversa administration for install the django administration jinja templating just add before django contrib admin in your installed apps in example configuration benchmarking running tests in django with jinja djinja performance using jinja instead of django templating in administration django debug toolbar for install the django debug toolbar jinja templating just add before debug toolbar in your installed apps in example configuration todos and bugs see", "reference_keywords": ""}, "pypa/packaging": {"git_readme": "packaging core utilities for python packages documentation discussion if you run into bugs you can file them in our you can also join on freenode to ask questions or get involved code of conduct everyone interacting in the packaging project codebases issue trackers chat rooms and mailing lists is expected to follow the pypa code of conduct", "pypi_description": "packaging core utilities for python packages documentation discussion if you run into bugs you can file them in our you can also join on freenode to ask questions or get involved code of conduct everyone interacting in the packaging project codebases issue trackers chat rooms and mailing lists is expected to follow the pypa code of conduct changelog improve error messages when invalid requirements are given drop support for python and define minimal pyparsing version to add and attributes to and add and to make it easy to determine if release is development release add to canonicalize version strings or instances add support for the deprecated marker which was an undocumented setuptools marker in addition to the newer markers fix regression in parsing requirements with whitespaces between the comma separators fix bug where was overly strict when matching legacy requirements implement requirement specifiers from pep normalize post release spellings for rev prefixes fix logic error that was causing inconsistent answers about whether or not pre release was contained within or not normalize release candidates as instead of expose the constant regular expression matching valid version backwards incompatible refactor specifier support so that it can sanely handle legacy specifiers as well as pep specifiers backwards incompatible move the specifier support out of into allow and to be sorted together add to enable easily parsing version string as either or depending on it pep validity initial release", "reference_keywords": "core utilities for python packages"}, "sumerc/yappi": {"git_readme": "yappi yet another python profiler but this time support multithread cpu time profiling motivation cpython standard distribution comes with three profilers and is implemented as c module based on is in pure python and can be seen as small subset of cprofile the major issue is that all of these profilers lack support for multi threaded programs and cpu time if you want to profile multi threaded application you must give an entry point to these profilers and then maybe merge the outputs none of these profilers are designed to work on long running multi threaded application it is impossible to profile an application retrieve the statistics then stop and then start later on the fly without affecting the profiled application highlights profiler can be started stopped at any time from any thread in the application profile statistics can be obtained from any thread at any time profile statistics can show actual cpu time used instead of wall time profiler pollution effect on the application run time is very minimal installation can be installed via pypi or from the source directly documentation introduction clock types api thanks features profiler results can be saved in callgrind or pstat formats new in profiler results can be merged from different sessions on the fly new in profiler results can be easily converted to pstats new in profiling of multithreaded python applications transparently supports profiling per thread cpu time new in profiler can be started from any thread at any time ability to get statistics at any time without even stopping the profiler various flags to arrange sort profiler results supports python limitations threads must be derived from threading module thread object talks python performance profiling the guts and the glory pycharm integration yappi is the default profiler in if you have yappi installed will use it see the official documentation for more details", "pypi_description": "yappi yet another python profiler but this time support multithread cpu time profiling motivation cpython standard distribution comes with three profilers and is implemented as c module based on is in pure python and can be seen as small subset of cprofile the major issue is that all of these profilers lack support for multi threaded programs and cpu time if you want to profile multi threaded application you must give an entry point to these profilers and then maybe merge the outputs none of these profilers are designed to work on long running multi threaded application it is impossible to profile an application retrieve the statistics then stop and then start later on the fly without affecting the profiled application highlights profiler can be started stopped at any time from any thread in the application profile statistics can be obtained from any thread at any time profile statistics can show actual cpu time used instead of wall time profiler pollution effect on the application run time is very minimal installation can be installed via pypi or from the source directly documentation introduction clock types api thanks features profiler results can be saved in callgrind or pstat formats new in profiler results can be merged from different sessions on the fly new in profiler results can be easily converted to pstats new in profiling of multithreaded python applications transparently supports profiling per thread cpu time new in profiler can be started from any thread at any time ability to get statistics at any time without even stopping the profiler various flags to arrange sort profiler results supports python limitations threads must be derived from threading module thread object talks python performance profiling the guts and the glory pycharm integration yappi is the default profiler in if you have yappi installed will use it see the official documentation for more details", "reference_keywords": "python thread multithread profiler"}, "whtsky/bencoder-pyx": {"git_readme": "", "pypi_description": "bencoder pyx fast bencode implementation in cython supports both python python image alt macos test status target image alt windows test status target image alt linux test status target image alt pypi license target image alt codecov coverage target image alt bencoder pyx pypi downloads target install code block bash usage code block python changelog version drop support for python performance boost for method version performance improvement fix package metainfo version use ordereddict instaed of dict support encoding subclasses of dict", "reference_keywords": "bencoding encode decode bittorrent bencode bencoder cython"}, "kelleyk/iterpipes3": {"git_readme": "", "pypi_description": "", "reference_keywords": ""}, "davidmarble/pywin": {"git_readme": "pywin pywin is lightweight python launcher and switcher for windows command line and msys mingw it similar to the but written with basic windows batch scripts and shell script for msys mingw support use bash and command line shell tools from based on msys mingw to do most of my python development on windows pywin supports several useful features defined in such as command line conventions and hash bang python version headers in source files while pywin lacks some of py exe features it has the basics and few extras of its own requirements windows xp for command prompt support windows vista for msys mingw support requires for symbolic links at least one installation of python up to though it not useful without at least two easy install pip or git installation make sure the main python installation you want pywin to live under is in the path with multiple python installations it recommended to have only the main and scripts directories from one installation in your path for example easy install pywin can create individual launch scripts to directly access core python installations g python these can be called directly with or without arguments from the windows command prompt and msys mingw bash prompt to use this feature call code sh launchers are created for all machine wide and user specific python installations found in the windows registry windows batch files will be added to the directory where pywin is located msys mingw relies on windows links created programmatically with which is why you must have windows vista installed to make use of this project launch specific python version using pywin add directive to the first or second line of source file to have the correct interpreter called currently this only supports python launchers created by the command to use this feature you must associate the py extension with pywin bat using the included utility code sh pywin note commands work from both windows command line and msys mingw shell genlaunchers code sh launch either specific version of python or source file or both note that specifying version of python on the command line will override any version in the header of the source file example code sh version search order original restore py registry settings to launch with all users if administrator apply changes to and remove any python keys note that users can override this with their own values pyhome the variable used by is set in this manner if the environment variable is set use it if the environment variable is set use it this is set when you call but only lasts for the current session the path the script is in", "pypi_description": "pywin pywin is lightweight python launcher and switcher for windows command line and msys mingw it similar to the but written with basic windows batch scripts and shell script for msys mingw support use bash and command line shell tools from based on msys mingw to do most of my python development on windows pywin supports several useful features defined in such as command line conventions and hash bang python version headers in source files while pywin lacks some of py exe features it has the basics and few extras of its own requirements windows xp for command prompt support windows vista for msys mingw support requires for symbolic links at least one installation of python up to though it not useful without at least two easy install pip or git installation make sure the main python installation you want pywin to live under is in the path with multiple python installations it recommended to have only the main and scripts directories from one installation in your path for example easy install pywin can create individual launch scripts to directly access core python installations g python these can be called directly with or without arguments from the windows command prompt and msys mingw bash prompt to use this feature call code sh launchers are created for all machine wide and user specific python installations found in the windows registry windows batch files will be added to the directory where pywin is located msys mingw relies on windows links created programmatically with which is why you must have windows vista installed to make use of this project launch specific python version using pywin add directive to the first or second line of source file to have the correct interpreter called currently this only supports python launchers created by the command to use this feature you must associate the py extension with pywin bat using the included utility code sh pywin note commands work from both windows command line and msys mingw shell genlaunchers code sh launch either specific version of python or source file or both note that specifying version of python on the command line will override any version in the header of the source file example code sh version search order original restore py registry settings to launch with all users if administrator apply changes to and remove any python keys note that users can override this with their own values pyhome the variable used by is set in this manner if the environment variable is set use it if the environment variable is set use it this is set when you call but only lasts for the current session the path the script is in", "reference_keywords": "python windows version switcher launcher"}, "perone/protocoin": {"git_readme": "protocoin bitcoin protocol pure python bitcoin protocol implementation documentation see the documentation for more information", "pypi_description": "a pure python bitcoin protocol implementation", "reference_keywords": "bitcoin protocol"}, "/flask-tryton": {"git_readme": "", "pypi_description": "flask tryton adds tryton support to flask application by default transactions are readonly except for put post delete and patch request methods it provides also routing converters and nutshell there are three configuration options available to report issues please visit the flask tryton bugtracker", "reference_keywords": "flask tryton web"}, "/contact-form": {"git_readme": "", "pypi_description": "", "reference_keywords": ""}, "JA-VON/qgen": {"git_readme": "qgen generate volume of questions based on template", "pypi_description": "generate random value of questions based on template", "reference_keywords": ""}, "uktrade/directory-validators": {"git_readme": "directory validators directory of uk exporters validators requirements installation usage development testing publish to pypi the package should be published to pypi on merge to master if you need to do it locally then get the credentials from rattic and add the environment variables to your host machine setting directory pypi username directory pypi password then run the following command", "pypi_description": "directory validators directory of uk exporters validators requirements installation usage development testing publish to pypi the package should be published to pypi on merge to master if you need to do it locally then get the credentials from rattic and add the environment variables to your host machine setting directory pypi username directory pypi password then run the following command", "reference_keywords": "live"}, "andreypopp/configure": {"git_readme": "yaml configuration library which provides interpolation for string configuration values configuration inheritance configuration composition object level configuration like construct this object by calling some function with some arguments development takes place at", "pypi_description": "yaml configuration library which provides interpolation for string configuration values configuration inheritance configuration composition object level configuration like construct this object by calling some function with some arguments development takes place at", "reference_keywords": ""}, "sontek/pyramid-celery": {"git_readme": "getting started include pyramid celery either by setting your includes in your ini or by calling code block ini then you just need to tell pyramid celery what ini file your celery section is in code block python then you are free to use celery for example class based code block python or decorator based code block python to get pyramid settings you may access them in configuration by default pyramid celery assumes you want to configure celery via an ini settings you can do this by calling config configure celery development ini but if you are already in the main of your application and want to use the ini used to configure the app you can do the following code block python if you want to use the standard celeryconfig python file you can set the use celeryconfig true like this code block ini you can get more information for celeryconfig py here an example ini configuration looks like this code block ini scheduled periodic tasks to use celerybeat periodic tasks you need to declare config section per task the options are task the python task you need executed type the type of scheduling your configuration uses options are and schedule the actual schedule for your of configuration args additional positional arguments kwargs additional keyword arguments example configuration for this code block ini gotcha you want to watchout for is that the date time in scheduled tasks is utc by default if you want to schedule for an exact date time for your local timezone you need to set documentation for that can be found here if you need to find out what timezones are available you can do the following code block python worker execution the celerybeat worker will read your configuration and schedule tasks in the queue to be executed at the time defined this means if you are using celerybeat you will end up running workers code block bash the first command is the standard worker command that will read messages off of the queue and run the task the second command will read the celerybeat configuration and periodically schedule tasks on the queue routing if you would like to route task to specific queue you can define route per task by declaring their and or in section an example configuration for this code block ini running the worker to run the worker we just use the standard celery command with an additional argument code block bash if you ve defined variables in your ini like database username you can use the ini var argument which is comma separated list of key value pairs code block bash the values in ini var cannot have spaces in them this will break celery parser the reason it is csv instead of using ini var multiple times is because of bug in celery itself when they fix the bug we will re work the api ticket is here if you use celerybeat scheduler you need to run with the beat flag to run beat and the worker at the same time code block bash or you can launch it separately like this code block bash logging if you use the ini configuration e don use celeryconfig py then the logging configuration will be loaded from the ini and will not use the default celery loggers you most likely want to add logging section to your ini for celery as well code block ini and then update your section to include it if you want use the default celery loggers then you can set celeryd hijack root logger true in the celery section of your ini celery worker processes do not propagade exceptions inside tasks but swallow them silently by default this is related to the behavior of reading asynchronous task results back to see if your tasks fail you might need to configure logger to propagate exceptions code block ini if you want use the default celery loggers then you can set celeryd hijack root logger true in the celery section of your ini demo to see it all in action check out examples long running with tm run redis server and then do code block bash", "pypi_description": "getting started include pyramid celery either by setting your includes in your ini or by calling code block ini then you just need to tell pyramid celery what ini file your celery section is in code block python then you are free to use celery for example class based code block python or decorator based code block python to get pyramid settings you may access them in configuration by default pyramid celery assumes you want to configure celery via an ini settings you can do this by calling config configure celery development ini but if you are already in the main of your application and want to use the ini used to configure the app you can do the following code block python if you want to use the standard celeryconfig python file you can set the use celeryconfig true like this code block ini you can get more information for celeryconfig py here an example ini configuration looks like this code block ini scheduled periodic tasks to use celerybeat periodic tasks you need to declare config section per task the options are task the python task you need executed type the type of scheduling your configuration uses options are and schedule the actual schedule for your of configuration args additional positional arguments kwargs additional keyword arguments example configuration for this code block ini gotcha you want to watchout for is that the date time in scheduled tasks is utc by default if you want to schedule for an exact date time for your local timezone you need to set documentation for that can be found here if you need to find out what timezones are available you can do the following code block python worker execution the celerybeat worker will read your configuration and schedule tasks in the queue to be executed at the time defined this means if you are using celerybeat you will end up running workers code block bash the first command is the standard worker command that will read messages off of the queue and run the task the second command will read the celerybeat configuration and periodically schedule tasks on the queue routing if you would like to route task to specific queue you can define route per task by declaring their and or in section an example configuration for this code block ini running the worker to run the worker we just use the standard celery command with an additional argument code block bash if you ve defined variables in your ini like database username you can use the ini var argument which is comma separated list of key value pairs code block bash the values in ini var cannot have spaces in them this will break celery parser the reason it is csv instead of using ini var multiple times is because of bug in celery itself when they fix the bug we will re work the api ticket is here if you use celerybeat scheduler you need to run with the beat flag to run beat and the worker at the same time code block bash or you can launch it separately like this code block bash logging if you use the ini configuration e don use celeryconfig py then the logging configuration will be loaded from the ini and will not use the default celery loggers you most likely want to add logging section to your ini for celery as well code block ini and then update your section to include it if you want use the default celery loggers then you can set celeryd hijack root logger true in the celery section of your ini celery worker processes do not propagade exceptions inside tasks but swallow them silently by default this is related to the behavior of reading asynchronous task results back to see if your tasks fail you might need to configure logger to propagate exceptions code block ini if you want use the default celery loggers then you can set celeryd hijack root logger true in the celery section of your ini demo to see it all in action check out examples long running with tm run redis server and then do code block bash support celery properly handle celery always eager handle tuple in admins", "reference_keywords": "paste pyramid celery message queue amqp job task distributed"}, "stackforge/winchester": {"git_readme": "", "pypi_description": "winchester an openstack notification event processing library based on persistant streams winchester is designed to process event streams such as those produced from openstack notifications events are represented as simple python dictionaries they should be flat dictionaries not nested with minimum of three keys the individual keys of the event dictionary are called traits and can be strings integers floats or datetimes for processing of the often large notifications that come out of openstack winchester uses the stackdistiller library to extract flattened events from the notifications that only contain the data you actually need for processing winchester processing is done through triggers and pipelines trigger is composed of match criteria which is like persistant query collecting events you want to process into persistant stream stored in sql database set of distinguishing traits which can separate your list of events into distinct streams similar to group by clause in an sql query and fire criteria which specifies the conditions given stream has to match for the trigger to fire when it does the events in the stream are sent to pipeline listed as the fire pipeline for processing as batch also listed is an expire timestamp if given stream does not meet the fire criteria by that time it is expired and can be sent to an expire pipeline for alternate processing both fire pipeline and expire pipeline are optional but at least one of them must be specified pipeline is simply list of simple handlers each handler in the pipeline receives the list of events in given stream sorted by timestamp in turn handlers can filter events from the list or add new events to it these changes will be seen by handlers further down the pipeline handlers should avoid operations with side effects other than modifying the list of events as pipeline processing can be re tried later if there is an error instead if all handlers process the list of events without raising an exception commit call is made on each handler giving it the chance to perform actions like sending data to external systems handlers are simple to write as pretty much any object that implements the appropriate handle events commit and rollback methods can be handler installing and running winchster is installable as simple python package once installed and the appropriate database url is specified in the winchester yaml config file example included in the etc directory you can create the appropriate database schema with if you need to run the sql by hand or just want to look at the schema the following will print out the appropriate table creation sql once you have done that and configured the appropriate triggers yaml pipelines yaml and if using stackdistiller event definitions yaml configs again examples are in etc in the winchester codebase you can add events into the system by calling the add event method of winchester triggermanager if you are processing openstack notifications you can call add notification which will pare down the notification into an event with stackdistiller and then call add event with that if you are reading openstack notifications off of rabbitmq queue there is plugin for the yagi notification processor included with winchester simply add winchester yagi handler winchesterhandler to the apps line in your yagi conf section for the queues you want to listen to and add section to the yagi conf to run the actual pipeline processing which is run as separate daemon run you can pass the flag to the pipeline worker to tell it to run as background daemon winchester uses an optimistic locking scheme in the database to coordinate firing expiring and processing of streams so you can run as many processes like yagi yagi event daemon feeding triggermanagers as you need to handle the incoming events and as many pipeline workers as you need to handle the resulting processing load scaling the system horizontally", "reference_keywords": "stacktach event processing pipeline events notification openstack triggers"}, "tnajdek/json-utils": {"git_readme": "", "pypi_description": "", "reference_keywords": ""}, "leplatrem/logging-color-formatter": {"git_readme": "logging color formatter alt latest pypi version alt latest travis ci build status colored logging formatter installation usage code block ini changes fix missing output of stacktraces on exceptions fix missing interpolation on strings using logger arguments run tests licence apache license v", "pypi_description": "logging color formatter alt latest pypi version alt latest travis ci build status colored logging formatter installation usage code block ini changes fix missing out of stacktraces on exceptions fix missing interpolation on strings using logger arguments run tests licence apache license v", "reference_keywords": "logging"}, "justanr/datestuff": {"git_readme": "datestuff for when you need some code helpers but not complete replacement for the modules why frankly love the built in code module almost everything need to do can just do with it however few things tend to creep up datetime and datetime again things like creating range of dates creating an unfixed date checking if two datetimes are within certain delta of one another here a short look at what included relativedate and relativedatetime these allow you to create an unfixed code or code instance by providing code offset and or factory method by default code uses code and code uses code as the default factories and both have default offset of code code block python however it is also possible to provide other factories as well code block python and as long as the underlying factory produces code or code compatible object everything will just work by compatible mean implements the code or code interface additionally if only static offset from today or now is desired you can simply provide the offset argument with code or dateutil code note that currently code and code are not interoperable code block python code and code also allow comparing against regular code and code instances with the standard operators etc making these incredibly useful for quickly defining date boundaries that are defined statically such as in serializer or orm model code block python adding and subtracting relative instances actually operate on their offsets rather than underlying code or code values code block python some alternate constructors are provided where it makes sense each allows passing an offset but defaults to code provided are code the default constructor code the default constructor allows passing tzinfo object to the factory code factory produces utc based datetimes note these are naive as it relies on the underlying code code the default constructor does not allow passing tzinfo object for convenience sake there are also truly static constructors code hoists regular date into relative context code hoists regular datetime into code hoists date into code context allows passing tzinfo object factory looks like code any additional static constructors such as code can be derived from these if truly needed code block python finally any functionality not implemented directly in the relative instance is proxied to the underlying code or code instance daterange range of dates is another tool find myself needing from time to time however eager creation can sometimes be very expensive for large range instead code is modeled after the python code type which has fast path lookup for membership lazy iteration indexing and slicing slices return new code objects code block python code also allows creating an open ended range by simply omitting the stop argument in this case the only functionality that will not work is using code and negative indexing slicing as there is no end currently code does not support code as under the hood it uses code for python and compatiblity this could be resolved in the future but is unlikely code is however compatible with code and code like objects and other code like objects interestingly this would apply to code and code as well utils currently the only util is code which is useful for comparing two code or code or like instances within certain delta code block python if simple boundary checking is needed this tool is much more light weight than either code or code sadly this is another tool that cannot interoperate with code as it and code are unorderable at least in python", "pypi_description": "datestuff for when you need some code helpers but not complete replacement for the modules why frankly love the built in code module almost everything need to do can just do with it however few things tend to creep up datetime and datetime again things like creating range of dates creating an unfixed date checking if two datetimes are within certain delta of one another here a short look at what included relativedate and relativedatetime these allow you to create an unfixed code or code instance by providing code offset and or factory method by default code uses code and code uses code as the default factories and both have default offset of code code block python however it is also possible to provide other factories as well code block python and as long as the underlying factory produces code or code compatible object everything will just work by compatible mean implements the code or code interface additionally if only static offset from today or now is desired you can simply provide the offset argument with code or dateutil code note that currently code and code are not interoperable code block python code and code also allow comparing against regular code and code instances with the standard operators etc making these incredibly useful for quickly defining date boundaries that are defined statically such as in serializer or orm model code block python adding and subtracting relative instances actually operate on their offsets rather than underlying code or code values code block python some alternate constructors are provided where it makes sense each allows passing an offset but defaults to code provided are code the default constructor code the default constructor allows passing tzinfo object to the factory code factory produces utc based datetimes note these are naive as it relies on the underlying code code the default constructor does not allow passing tzinfo object for convenience sake there are also truly static constructors code hoists regular date into relative context code hoists regular datetime into code hoists date into code context allows passing tzinfo object factory looks like code any additional static constructors such as code can be derived from these if truly needed code block python finally any functionality not implemented directly in the relative instance is proxied to the underlying code or code instance daterange range of dates is another tool find myself needing from time to time however eager creation can sometimes be very expensive for large range instead code is modeled after the python code type which has fast path lookup for membership lazy iteration indexing and slicing slices return new code objects code block python code also allows creating an open ended range by simply omitting the stop argument in this case the only functionality that will not work is using code and negative indexing slicing as there is no end currently code does not support code as under the hood it uses code for python and compatiblity this could be resolved in the future but is unlikely code is however compatible with code and code like objects and other code like objects interestingly this would apply to code and code as well utils currently the only util is code which is useful for comparing two code or code or like instances within certain delta code block python if simple boundary checking is needed this tool is much more light weight than either code or code sadly this is another tool that cannot interoperate with code as it and code are unorderable at least in python the mit license mit copyright alec nikolas reiter permission is hereby granted free of charge to any person obtaining copy of this software and associated documentation files the software to deal in the software without restriction including without limitation the rights to use copy modify merge publish distribute sublicense and or sell copies of the software and to permit persons to whom the software is furnished to do so subject to the following conditions the above copyright notice and this permission notice shall be included in all copies or substantial portions of the software the software is provided as is without warranty of any kind express or implied including but not limited to the warranties of merchantability fitness for particular purpose and noninfringement in no event shall the authors or copyright holders be liable for any claim damages or other liability whether in an action of contract tort or otherwise arising from out of or in connection with the software or the use or other dealings in the software", "reference_keywords": "dates datetime"}, "mythonlang/pgen2": {"git_readme": "pypgen python implementation of the python parser generator pgen", "pypi_description": "", "reference_keywords": "python implementation of the python parser generator pgen  parser generator"}, "/mopidy": {"git_readme": "", "pypi_description": "mopidy mopidy is an extensible music server written in python mopidy plays music from local disk spotify soundcloud google play music and more you edit the playlist from any phone tablet or computer using range of mpd and web clients stream music from the cloud vanilla mopidy only plays music from your local disk and radio streams through extensions mopidy can play music from cloud services like spotify soundcloud and google play music with mopidy extension support backends for new music sources can be easily added mopidy is just server mopidy is python application that runs in terminal or in the background on linux computers or macs that have network connectivity and audio output out of the box mopidy is an mpd and http server additional frontends for controlling mopidy can be installed from extensions everybody use their favorite client you and the people around you can all connect their favorite mpd or web client to the mopidy server to search for music and manage the playlist together with browser or mpd client which is available for all popular operating systems you can control the music from any phone tablet or computer mopidy on raspberry pi the raspberry pi is popular device to run mopidy on either using raspbian or arch linux it is quite slow but it is very affordable in fact the kickstarter funded gramofon modern cloud jukebox project used mopidy on raspberry pi to prototype the gramofon device mopidy is also major building block in the pi musicbox integrated audio jukebox system for raspberry pi mopidy is hackable mopidy extension support and python json rpc and javascript apis make mopidy perfect for building your own hacks in one project raspberry pi was embedded in an old cassette player the buttons and volume control are wired up with gpio on the raspberry pi and is used to control playback through custom mopidy extension the cassettes have nfc tags used to select playlists from spotify to get started with mopidy check out irc at announcement list twitter alt latest pypi version alt travis ci build status alt test coverage", "reference_keywords": ""}, "requests/requests-oauthlib": {"git_readme": "requests oauthlib build status coverage status docs this project provides first class oauth library support for the oauth workflow oauth can seem overly complicated and it sure has its quirks luckily requests oauthlib hides most of these and let you focus at the task at hand accessing protected resources using requests oauthlib is as simple as code block pycon before accessing resources you will need to obtain few credentials from your provider g twitter and authorization from the user for whom you wish to retrieve resources for you can read all about this in the full the oauth workflow oauth is generally simpler than oauth but comes in more flavours the most common being the authorization code grant also known as the webapplication flow fetching protected resource after obtaining an access token can be extremely simple however before accessing resources you will need to obtain few credentials from your provider g google and authorization from the user for whom you wish to retrieve resources for you can read all about this in the full installation to install requests and requests oauthlib you can use pip code block bash build status coverage status docs image alt documentation status scale target", "pypi_description": "requests oauthlib build status coverage status docs this project provides first class oauth library support for the oauth workflow oauth can seem overly complicated and it sure has its quirks luckily requests oauthlib hides most of these and let you focus at the task at hand accessing protected resources using requests oauthlib is as simple as code block pycon before accessing resources you will need to obtain few credentials from your provider g twitter and authorization from the user for whom you wish to retrieve resources for you can read all about this in the full the oauth workflow oauth is generally simpler than oauth but comes in more flavours the most common being the authorization code grant also known as the webapplication flow fetching protected resource after obtaining an access token can be extremely simple however before accessing resources you will need to obtain few credentials from your provider g google and authorization from the user for whom you wish to retrieve resources for you can read all about this in the full installation to install requests and requests oauthlib you can use pip code block bash build status coverage status docs image alt documentation status scale target history unreleased nothing yet january this project now depends on oauthlib and above it does not support versions of oauthlib before updated oauth tests to use sess for an oauthsession instance instead of because oauthsession objects and methods acceept an paramether which is typically an instance of previously tried to guess how and where to provide client and user credentials incorrectly this was incompatible with some oauth servers and incompatible with breaking changes in oauthlib that seek to correctly provide the the older implementation also did not raise the correct exceptions when username and password are not present on legacy clients avoid automatic netrc authentication for oauthsession january adjusted version specifier for dependency this project is not yet compatible with dropped dependency on minor changes to clean up the code and make it more readable maintainable june removed support for python and python this project now supports python and python and above added several examples to the documentation added plentymarkets compliance fix added property to oauthsession to match the corresponding property on oauthsession february added fitbit compliance fix fixed an issue where newlines in the response body for the access token request would cause errors when trying to extract the token fixed an issue introduced in where users passing to several methods would encounter conflicts with the and derived auth the user supplied argument is now used in preference to those options september allowed to take the and parameters for the purposes of automatic token refresh which may need them july use and for the authorization header if provided allow explicit bypass of the authorization header by setting pass through the kwarg when refreshing tokens miscellaneous cleanups february fixed bug when sending authorization in headers with no username and password present make sure we clear the session token before obtaining new one some improvements to the slack compliance fix avoid timing problems around token refresh allow passing arbitrary arguments to requests when calling and december add compliance fix for slack add compliance fix for mailchimp exceptions now carry the entire response not just the status code pass through keyword arguments when refreshing tokens automatically send authorization in headers not just body to maximize compatibility more getters setters available for oauth session client values allow sending custom headers when refreshing tokens and set some defaults may fix being raised instead of error raise requests exceptions on xx and xx responses in the oauth flow avoid when initializing the class without complete client information october new property on oauthsession and oauthsession which allows you to easily determine if the session is already authorized with oauth tokens or not new and exception classes for oauthsession this will make it easier to catch and identify these exceptions june new install target for people using oauth rsa sha signature method fixed bug in oauth where supplied state param was not used in auth url oauth https checking can be disabled by setting environment variable oauth now re authorize upon redirects oauth token fetching now raise detailed error message when the response body is incorrectly encoded or the request was denied added support for custom oauth clients oauth compliance fix for sina weibo multiple fixes to facebook compliance fix compliance fixes now re encode body properly as bytes in python logging now properly done under namespace instead of piggybacking on oauthlib namespace logging introduced for oauth auth and session september oauthsession methods only return unicode strings renamed requests oauthlib core to requests oauthlib oauth auth for consistency added facebook compliance fix and access token response hook to oauthsession added linkedin compliance fix added refresh token response compliance hook invoked before parsing the refresh token correctly limit compliance hooks to running only once content type guessing should only be done when no content type is given oauth now updates headers instead of replacing it with non case insensitive dict remove last use of response content in oauthsession state param can now be supplied in oauthsession authorize url", "reference_keywords": "oauthlib support for python requests python python requests oauth client oauth client"}, "xeroc/uptick": {"git_readme": "uptick swiss army knife for interacting with the bitshares blockchain stable develop documentation the full length documentation can be found on installation contributing uptick welcomes contributions from anyone and everyone please see our guidelines for contributing and the code of conduct discussion and developers discussions around development and use of this library can be found in dedicated telegram channel license copy of the license is available in the repository license file", "pypi_description": "uptick swiss army knife for interacting with the bitshares blockchain stable develop documentation the full length documentation can be found on installation contributing uptick welcomes contributions from anyone and everyone please see our guidelines for contributing and the code of conduct discussion and developers discussions around development and use of this library can be found in dedicated telegram channel license copy of the license is available in the repository license file", "reference_keywords": "bitshares library api rpc cli"}, "amiorin/blaz": {"git_readme": "intro blaz runs your scripts inside docker why with blaz you can create docker images with all the dependencies of your script like python ansible aws cli terraform puppet chef make go requirements linux osx docker machine dinghy works out of the box or static version of docker nfs better or vboxsf you could have stale scripts in osx docker image with python and blaz docker machine dinghy works out of the box quick start environment variables these are the defaults that you can override all environment variables like and are forwarded to the next container the former are printed the latter are not useful for secrets like aws credentials inside jenkins the strings inside the environment variable and are forwarded as environment variables reserved env variables and are reserved env var explanation blaz lock it the digest of the fullpath of the script and it used to understand if we need to start new blaz version for debugging purpose it the blaz version inside the container blaz skip when you want to compose two blaz scripts but you don want to start two different containers blaz chdir rel when the script has to access to files that are not under his directory but somewhere else it allows mount volume that is different from the directory of the current script using relative path like blaz dont pull it used in development mode blaz vars every env var starting with like or contains list of custom environment variables to be forwarded docker options to override docker exe to specify the docker executable when you have multiple versions docker sock to override the nested scripts blaz script can invoke another blaz script new docker container will be used for the nested script unless you define the environment variable blaz api pull blaz always pulls the docker image this allows you to use latest in your jenkins script and improve your image without making new commits to your project use to disable this behaviour use cases jenkins ansible build docker images build docker images you can split compile and build for example you can create script that compile your go source code with alpine blaz go mb build alpine docker with the static go executable mb your go program push to the docker container registry ansible entrypoint trick wrapper script can fix the user permissions and check the version of blaz dockerfile jessie contains an example in python publish development install pyenv", "pypi_description": "", "reference_keywords": ""}, "edsu/wikidata-suggest": {"git_readme": "wikidata suggest is simple command line tool for interactively reconciling your data against wikidata first you ll want to install once you ve installed it you will get command line tool most likely you will want to use wikidata suggest as little data cleansing augmentation library for example if you have csv spreadsheet that has an author column that you like to link up to wikidata you can do something like this", "pypi_description": "", "reference_keywords": ""}, "/jupyterhub": {"git_readme": "", "pypi_description": "technical overview installation configuration docker contributing license help and resources jupyterhub with jupyterhub you can create multi user hub which spawns manages and proxies multiple instances of the single user jupyter notebook server project jupyter created jupyterhub to support many users the hub can offer notebook servers to class of students corporate data science workgroup scientific research project or high performance computing group technical overview three main actors make up jupyterhub multi user hub tornado process configurable http proxy node http proxy multiple single user jupyter notebook servers python jupyter tornado basic principles for operation are hub launches proxy proxy forwards all requests to hub by default hub handles login and spawns single user servers on demand hub configures proxy to forward url prefixes to the single user notebook servers jupyterhub also provides rest api for administration of the hub and its users installation check prerequisites linux unix based system python or greater nodejs npm if you are using the nodejs and npm dependencies will be installed for you by conda if you are using install recent version of nodejs npm for example install it on linux debian ubuntu using the package installs the executable and is currently required for npm to work on debian ubuntu tls certificate and key for https communication domain name install packages using to install jupyterhub along with its dependencies including nodejs npm if you plan to run notebook servers locally install the jupyter notebook or jupyterlab using jupyterhub can be installed with and the proxy with if you plan to run notebook servers locally you will need to install the jupyter notebook package run the hub server to start the hub server run the command visit in your browser and sign in with your unix pam credentials note to allow multiple users to sign into the server you will need to run the command as privileged user such as root the wiki describes how to run the server as less privileged user which requires more configuration of the system configuration the getting started section of the documentation explains the common steps in setting up jupyterhub the jupyterhub tutorial provides an in depth video and sample configurations of jupyterhub create configuration file to generate default config file with settings and descriptions start the hub to start the hub on specific url and port with https authenticators authenticator description pamauthenticator default built in authenticator oauthenticator oauth jupyterhub authenticator oauthenticator ldapauthenticator simple ldap authenticator plugin for jupyterhub kerberosauthenticator kerberos authenticator plugin for jupyterhub spawners spawner description localprocessspawner default built in spawner starts single user servers as local processes dockerspawner spawn single user servers in docker containers kubespawner kubernetes spawner for jupyterhub sudospawner spawn single user servers without being root systemdspawner spawn single user notebook servers using systemd batchspawner designed for clusters using batch scheduling software yarnspawner spawn single user notebook servers distributed on hadoop cluster wrapspawner wrapspawner and profilesspawner enabling runtime configuration of spawners docker starter docker image for jupyterhub gives baseline deployment of jupyterhub using docker important this image contains only the hub itself with no configuration in general one needs to make derivative image with at least setting up an authenticator and or spawner to run the single user servers which may be on the same system as the hub or not jupyter notebook version or greater must be installed the jupyterhub docker image can be started with the following command this command will create container named that you can stop and resume with the hub service will be listening on all interfaces at port which makes this good choice for testing jupyterhub on your desktop or laptop if you want to run docker on computer that has public ip then you should as in must secure it with ssl by adding ssl options to your docker configuration or by using ssl enabled proxy mounting volumes will allow you to store data outside the docker image host system so it will be persistent even when you start new image the command will spawn root shell in your docker container you can use the root shell to create system users in the container these accounts will be used for authentication in jupyterhub default configuration contributing if you would like to contribute to the project please read our contributor documentation and the the file explains how to set up development installation how to run the test suite and how to contribute to documentation for high level view of the vision and next directions of the project see the jupyterhub community roadmap note about platform support jupyterhub is supported on linux unix based systems jupyterhub officially does not support windows you may be able to use jupyterhub on windows if you use spawner and authenticator that work on windows but the jupyterhub defaults will not bugs reported on windows will not be accepted and the test suite will not run on windows small patches that fix minor windows compatibility issues such as basic installation may be accepted however for windows based systems we would recommend running jupyterhub in docker container or linux vm additional reference tornado documentation on windows platform support license we use shared copyright model that enables all contributors to maintain the copyright on their contributions all code is licensed under the terms of the revised bsd license help and resources we encourage you to ask questions on the jupyter mailing list to participate in development discussions or get help talk with us on our jupyterhub gitter channel reporting issues jupyterhub tutorial documentation for jupyterhub pdf latest pdf stable documentation for jupyterhub rest api documentation for project jupyter pdf project jupyter website jupyterhub follows the jupyter community guides technical overview installation configuration docker contributing license help and resources", "reference_keywords": "interactive interpreter shell web"}}